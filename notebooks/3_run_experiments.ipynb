{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c14ebe42",
   "metadata": {},
   "source": [
    "### Carga de Datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55a1fbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 908 | Val: 114 | Test: 114\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "sys.path.append('../') \n",
    "\n",
    "# Cargar datos procesados\n",
    "DATA_PATH = '../data/processed/'\n",
    "train_df = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\n",
    "val_df = pd.read_csv(os.path.join(DATA_PATH, 'validation.csv'))\n",
    "test_df = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))\n",
    "\n",
    "# Separar X e y\n",
    "X_train, y_train = train_df['text'].tolist(), train_df['manual_classification'].tolist()\n",
    "X_val, y_val = val_df['text'].tolist(), val_df['manual_classification'].tolist()\n",
    "X_test, y_test = test_df['text'].tolist(), test_df['manual_classification'].tolist()\n",
    "\n",
    "print(f\"Train: {len(X_train)} | Val: {len(X_val)} | Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf64ecc2",
   "metadata": {},
   "source": [
    "### **Experimento 1.1:** Limpieza Mínima + TF-IDF + Regresión Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5b452a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.preprocessing_utils:Modelo spaCy cargado correctamente\n",
      "INFO:src.preprocessing_utils:Descargando recurso NLTK: stopwords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando Exp 1.1 (Baseline) ---\n",
      "Entrenando Exp 1.1...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.74      0.75        70\n",
      "           1       0.61      0.64      0.62        44\n",
      "\n",
      "    accuracy                           0.70       114\n",
      "   macro avg       0.69      0.69      0.69       114\n",
      "weighted avg       0.70      0.70      0.70       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.baseline_model import run_exp_1_1\n",
    "\n",
    "print(\"--- Iniciando Exp 1.1 (Baseline) ---\")\n",
    "pipeline_1_1 = run_exp_1_1(X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026641ab",
   "metadata": {},
   "source": [
    "### **Experimento 2.1:** Limpieza Mínima + BERT + Regresión Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38240c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\Desktop\\TT\\proyecto-transformacion-texto-imagen\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando Exp 2.1 (BERT Frozen) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrayendo embeddings de Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo Embeddings:   0%|          | 0/29 [00:00<?, ?it/s]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Extrayendo Embeddings: 100%|██████████| 29/29 [01:48<00:00,  3.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrayendo embeddings de Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo Embeddings: 100%|██████████| 4/4 [00:13<00:00,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.71      0.71        70\n",
      "           1       0.55      0.55      0.55        44\n",
      "\n",
      "    accuracy                           0.65       114\n",
      "   macro avg       0.63      0.63      0.63       114\n",
      "weighted avg       0.65      0.65      0.65       114\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from features.bert_extractor import BertFeatureExtractor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"--- Iniciando Exp 2.1 (BERT Frozen) ---\")\n",
    "\n",
    "# 1. Inicializar el extractor (Esto descarga el modelo si no está en cache)\n",
    "extractor = BertFeatureExtractor(model_name=\"dccuchile/bert-base-spanish-wwm-uncased\")\n",
    "\n",
    "# 2. Convertir texto a embeddings (Esto puede tardar unos minutos)\n",
    "print(\"Extrayendo embeddings de Train...\")\n",
    "X_train_emb = extractor.get_embeddings(X_train)\n",
    "\n",
    "print(\"Extrayendo embeddings de Test...\")\n",
    "X_test_emb = extractor.get_embeddings(X_test)\n",
    "\n",
    "# 3. Entrenar clasificador ligero sobre los embeddings\n",
    "clf_2_1 = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "clf_2_1.fit(X_train_emb, y_train)\n",
    "\n",
    "# 4. Evaluar\n",
    "preds_2_1 = clf_2_1.predict(X_test_emb)\n",
    "print(classification_report(y_test, preds_2_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbf14f9",
   "metadata": {},
   "source": [
    "### **Experimento 3.1:** Limpieza Mínima + BERT (Fine-Tuning) + Regresión Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffca8825",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
